{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "needed-accounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cross-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-trauma",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "palestinian-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data'\n",
    "HEADERS = ['age', 'sex', 'cp', 'restbp', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
    "df = pd.read_csv(URL, names = HEADERS)\n",
    "train_test_df, val_df = train_test_split(df, test_size=0.1, stratify=df.target, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-television",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "overhead-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(preproc_df):\n",
    "    bin_cols = {'thal': 3.0, 'cp': 4.0, 'restecg':0.0}\n",
    "    cat_cols = ['slope', 'ca']\n",
    "    \n",
    "    preproc_df = preproc_df.apply(lambda x: x.replace(['?'], np.nan), axis=1)\n",
    "    preproc_df.dropna()\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        new_cols = pd.get_dummies(preproc_df[col], prefix=col)\n",
    "        preproc_df = pd.concat([preproc_df, new_cols], axis=1)\n",
    "        preproc_df = preproc_df.drop(col, axis=1)\n",
    "        \n",
    "    for col in bin_cols.keys():\n",
    "        preproc_df[col] = preproc_df[col].apply(lambda x: 0 if float(x) == bin_cols[col] else 1)\n",
    "    \n",
    "    preproc_df.target = preproc_df.target.apply(lambda x: 0 if x == 0 else 1)\n",
    "    return preproc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "isolated-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_df = preprocessing(train_test_df)\n",
    "val_df = preprocessing(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-opposition",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "minute-zoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_set(set_df, minutes):         \n",
    "    X = set_df[get_feat_list(minutes)]\n",
    "    y = set_df.target\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "editorial-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feat_list(minutes):\n",
    "    times_available = time_constraints.keys()\n",
    "    feat_list = []\n",
    "    \n",
    "    for limit in times_available:\n",
    "        if limit <= minutes: feat_list += time_constraints[limit]\n",
    "    \n",
    "    return feat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-safety",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cardiovascular-recall",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X_train, X_test, y_train, y_test, X_val, y_val, minutes):\n",
    "    model = LogisticRegression(C=0.5, solver='liblinear')\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    importance = model.coef_[0]\n",
    "    import_by_featname = sorted(list(zip(X_train.columns, importance)), key=lambda x: x[1])\n",
    "    \n",
    "    print('----Test----')\n",
    "    predictions = model.predict(X_test)\n",
    "    print(classification_report(y_test, predictions, target_names=['absense', 'presence']))\n",
    "    \n",
    "    print('----Validation----')\n",
    "    predictions = model.predict(X_val)\n",
    "    print(classification_report(y_val, predictions, target_names=['absense', 'presence']))\n",
    "    \n",
    "    pickle.dump(model, open(f'models/model.{minutes}.sav', 'wb'))\n",
    "    \n",
    "    return import_by_featname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "spatial-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_feat_importance(importance, minutes):\n",
    "    features = [x[0] for x in importance]\n",
    "    importance = [x[1] for x in importance]\n",
    "\n",
    "    plt.bar(features, importance)\n",
    "    plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "    plt.title(f'Feature importance for diagnostic tests under {minutes} minutes')\n",
    "    plt.savefig(f'images/feature_importance_<{minutes}.png', dpi=100)\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-admission",
   "metadata": {},
   "source": [
    "# Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "mobile-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_constraints = {\n",
    "    60:['age', 'sex', 'cp', 'restbp', 'chol', 'restecg', 'thalach', 'thal'], \n",
    "    120:['ca_0.0', 'ca_1.0', 'ca_2.0', 'ca_3.0'], \n",
    "    600:['fbs'], \n",
    "    720:['exang', 'oldpeak', 'slope_1.0', 'slope_2.0', 'slope_3.0']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "olympic-middle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Building model for diagnostic tools that take under 60 minutes to perform----\n",
      "----Test----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     absense       0.83      0.83      0.83        30\n",
      "    presence       0.80      0.80      0.80        25\n",
      "\n",
      "    accuracy                           0.82        55\n",
      "   macro avg       0.82      0.82      0.82        55\n",
      "weighted avg       0.82      0.82      0.82        55\n",
      "\n",
      "----Validation----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     absense       0.78      0.82      0.80        17\n",
      "    presence       0.77      0.71      0.74        14\n",
      "\n",
      "    accuracy                           0.77        31\n",
      "   macro avg       0.77      0.77      0.77        31\n",
      "weighted avg       0.77      0.77      0.77        31\n",
      "\n",
      "-----Building model for diagnostic tools that take under 120 minutes to perform----\n",
      "----Test----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     absense       0.86      0.83      0.85        30\n",
      "    presence       0.81      0.84      0.82        25\n",
      "\n",
      "    accuracy                           0.84        55\n",
      "   macro avg       0.83      0.84      0.84        55\n",
      "weighted avg       0.84      0.84      0.84        55\n",
      "\n",
      "----Validation----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     absense       0.83      0.88      0.86        17\n",
      "    presence       0.85      0.79      0.81        14\n",
      "\n",
      "    accuracy                           0.84        31\n",
      "   macro avg       0.84      0.83      0.84        31\n",
      "weighted avg       0.84      0.84      0.84        31\n",
      "\n",
      "-----Building model for diagnostic tools that take under 600 minutes to perform----\n",
      "----Test----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     absense       0.86      0.83      0.85        30\n",
      "    presence       0.81      0.84      0.82        25\n",
      "\n",
      "    accuracy                           0.84        55\n",
      "   macro avg       0.83      0.84      0.84        55\n",
      "weighted avg       0.84      0.84      0.84        55\n",
      "\n",
      "----Validation----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     absense       0.83      0.88      0.86        17\n",
      "    presence       0.85      0.79      0.81        14\n",
      "\n",
      "    accuracy                           0.84        31\n",
      "   macro avg       0.84      0.83      0.84        31\n",
      "weighted avg       0.84      0.84      0.84        31\n",
      "\n",
      "-----Building model for diagnostic tools that take under 720 minutes to perform----\n",
      "----Test----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     absense       0.86      0.83      0.85        30\n",
      "    presence       0.81      0.84      0.82        25\n",
      "\n",
      "    accuracy                           0.84        55\n",
      "   macro avg       0.83      0.84      0.84        55\n",
      "weighted avg       0.84      0.84      0.84        55\n",
      "\n",
      "----Validation----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     absense       0.83      0.88      0.86        17\n",
      "    presence       0.85      0.79      0.81        14\n",
      "\n",
      "    accuracy                           0.84        31\n",
      "   macro avg       0.84      0.83      0.84        31\n",
      "weighted avg       0.84      0.84      0.84        31\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for time_limit in time_constraints.keys():\n",
    "    print(f'-----Building model for diagnostic tools that take under {time_limit} minutes to perform----')\n",
    "    \n",
    "    X, y = create_set(set_df=train_test_df, minutes=time_limit)\n",
    "    X_val, y_val = create_set(set_df=val_df, minutes=time_limit)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=train_test_df.target, random_state=1234)\n",
    "    \n",
    "    feature_weights = build_model(X_train, X_test, y_train, y_test, X_val, y_val, time_limit)\n",
    "    save_feat_importance(feature_weights, time_limit)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
